{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import praw\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='R30fcxAZLJ7Wyw',\n",
    "                     client_secret='8Zf87krHYa_zXRXPWEF8hK2Twfs',\n",
    "                     user_agent='project1'\n",
    "                    )\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs={0:'Politics',1:'Non-Political',2:'AskIndia', 3:\"[R]eddiquette\", 4:'Science/Technology', 5:'Policy/Economy', 6:'Business/Finance', 7:'Scheduled', 8:'Sports', 9:'Food', 10:'Photography', 11:'AMA', 12:'Coronavirus'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = ''\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            if w.isalnum():\n",
    "                w = w.lower()\n",
    "                filtered_sentence = filtered_sentence + ' ' + w\n",
    "    filtered_sentence = \" \".join(filtered_sentence.split())\n",
    "    return(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flair(url,loaded_model):\n",
    "\n",
    "    submission = reddit.submission(url=url)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data['title'] = submission.title\n",
    "    data['url'] = submission.url\n",
    "\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "        comment = comment + ' ' + top_level_comment.body\n",
    "    data[\"comment\"] = comment\n",
    "    data['title'] = preprocess_input(data['title'])\n",
    "    data['comment'] = preprocess_input(data['comment'])\n",
    "    data['combine'] = data['title'] + ' ' + data['comment']\n",
    "\n",
    "    return flairs[loaded_model.predict([data['title']])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(\"../model/tmodel.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business/Finance'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/india/comments/g62sa1/i_find_this_extremely_strange_album_on_imgur_some/\"\n",
    "detect_flair(url, loaded_model )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
